{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modified resnet50.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqq3eJ93VavJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/gdrive',force_remount=True)\n",
        "  root_path = 'gdrive/My Drive/data1/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_yKOdXNXndK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htzjS3Y-u3Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Z1=[]\n",
        "P1=[]\n",
        "\n",
        "for q in range(100,131):\n",
        "  filname = root_path + str(q) + \".mat\"\n",
        "  #import\n",
        "  mat = scipy.io.loadmat(filname)['Ftr']\n",
        "  L1=list(mat)\n",
        "\n",
        "  # LRFs\n",
        "  for i in range(0,500):\n",
        "    \n",
        "    K=np.ndarray.tolist(np.asarray(L1[0][0][i][0]))\n",
        "    Z1.append(K)\n",
        "  # print(Z1)\n",
        "\n",
        "  # Neighbours\n",
        "  for i in range(0,500):\n",
        "    \n",
        "    K=np.ndarray.tolist(np.asarray(L1[0][1][i][0]))\n",
        "    P1.append(K)\n",
        "  # print(P1)\n",
        "\n",
        "  \n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTMSyoVshcHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for q in range(131,160):\n",
        "  filname = root_path + str(q) + \".mat\"\n",
        "  #import\n",
        "  mat = scipy.io.loadmat(filname)['Ftr']\n",
        "  L1=list(mat)\n",
        "\n",
        "  # LRFs\n",
        "  for i in range(0,500):\n",
        "    \n",
        "    K=np.ndarray.tolist(np.asarray(L1[0][0][i][0]))\n",
        "    Z1.append(K)\n",
        "  # print(Z1)\n",
        "\n",
        "  # Neighbours\n",
        "  for i in range(0,500):\n",
        "    \n",
        "    K=np.ndarray.tolist(np.asarray(L1[0][1][i][0]))\n",
        "    P1.append(K)\n",
        "  # print(P1)\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr3AnzGfsslr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "P1=np.asarray(P1)\n",
        "Z1=np.asarray(Z1)\n",
        "\n",
        "print(len(P1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mawPxoLWKPmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sess = tf.Session()\n",
        "print(sess.run(main_loss(a,b)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL1ukb9cu0hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(P1,Z1,test_size=0.4,random_state=0)\n",
        "\n",
        "x_train=np.float32(x_train)\n",
        "\n",
        "x_test=np.float32(x_test)\n",
        "\n",
        "y_train=np.float32(y_train)\n",
        "\n",
        "y_test=np.float32(y_test)\n",
        "\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU1hV24cOUJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You will learn how to build very deep convolutional networks, using Residual Networks (ResNets)\n",
        "# In theory, very deep networks can represent very complex functions; but in practice, they are hard to train. Residual Networks, introduced by He et al., allow you to train much deeper networks than were previously practically feasible.\n",
        "\n",
        "# Let's import packages\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Dropout ,Activation, ZeroPadding1D, BatchNormalization, Flatten, SeparableConv1D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "#from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "\n",
        "# Identity block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = SeparableConv1D(filters = F1, kernel_size = 1, strides = 1, padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    \n",
        "    # Second component of main path \n",
        "    X = SeparableConv1D(filters = F2, kernel_size=(f), strides = (1), padding='same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = SeparableConv1D(filters = F3, kernel_size=(1), strides = 1, padding=\"valid\", name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization( name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLuM6yqq51B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
        "    '''conv_block is the block that has a conv layer at shortcut\n",
        "    # Arguments\n",
        "        input_tensor: input tensor\n",
        "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
        "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
        "        stage: integer, current stage label, used for generating layer names\n",
        "        block: 'a','b'..., current block label, used for generating layer names\n",
        "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
        "    And the shortcut should have subsample=(2,2) as well\n",
        "    '''\n",
        "        eps = 1.1e-5\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n",
        "                      name=conv_name_base + '2a', bias=False)(input_tensor)\n",
        "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
        "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
        "\n",
        "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
        "    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n",
        "                      name=conv_name_base + '2b', bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
        "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
        "\n",
        "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c', bias=False)(x)\n",
        "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
        "\n",
        "    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n",
        "                             name=conv_name_base + '1', bias=False)(input_tensor)\n",
        "    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
        "    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)\n",
        "\n",
        "    x = merge([x, shortcut], mode='sum', name='res' + str(stage) + block)\n",
        "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvDlVAjDq5vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def ResNet50():\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    SeparableConv1D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "X_input=Input(shape=(1499,3))\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "X = ZeroPadding1D((3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "X = SeparableConv1D(16, (7), strides = (2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "X = BatchNormalization(name = 'bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling1D((4), strides=(4))(X)\n",
        "\n",
        "    # Stage 2\n",
        "X = convolutional_block(X, f = 3, filters = [16, 16, 32], stage = 2, block='a', s=1)\n",
        "X = identity_block(X, 3, [16, 16, 32], stage=2, block='b')\n",
        "X = identity_block(X, 3, [16, 16, 32], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "X = convolutional_block(X, f=3, filters = [32,32,64], stage = 3, block='a', s=2)\n",
        "X = identity_block(X, 3, filters = [32,32,64],stage=3, block='b')\n",
        "X = identity_block(X, 3, filters = [32,32,64], stage=3, block='c')\n",
        "X = identity_block(X, 3, filters = [32,32,64], stage =3, block='d')\n",
        "X = AveragePooling1D((3), name='avg_pool0')(X)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # Stage 4 \n",
        "X = convolutional_block(X, f=3, filters = [64,64,128],stage=4, block='a', s=2)\n",
        "X = identity_block(X, 3, filters = [64,64,128], stage=4, block='b')\n",
        "X = identity_block(X, 3, filters = [64,64,128], stage=4, block='c')\n",
        "X = identity_block(X, 3, filters= [64,64,128], stage=4, block='d')\n",
        "X = identity_block(X, 3, filters=[64,64,128], stage=4, block='e')\n",
        "X = identity_block(X, 3, filters=[64,64,128], stage=4, block='f')\n",
        "\n",
        "    # Stage 5 \n",
        "X = convolutional_block(X, f=3, filters=[128,128,256], stage=5,block='a', s=3)\n",
        "X = identity_block(X, 3, filters=[128,128,256], stage=5, block='b')\n",
        "X = identity_block(X,3, filters=[128,128,256], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling1D(...)(X)\"\n",
        "X = AveragePooling1D((2), name='avg_pool')(X)\n",
        "    \n",
        "#X=Dropout(0.38)(X)\n",
        "    # output layer\n",
        "   # X = Flatten()(X)\n",
        "X = Dense(3, activation='linear', name='fc' + str(3), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "   \n",
        "   # return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N729VbtEQCFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import adam\n",
        "# Run the following code to build the model's graph. If your implementation is not correct you will know it by checking your accuracy when running model.fit(...) below.\n",
        "# model = ResNet50()\n",
        "opt = adam(lr=0.1,clipnorm=1.0)\n",
        "\n",
        "model.compile(loss='cosine_proximity', optimizer= opt ,metrics=['accuracy'])\n",
        "model.summary()\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "history = model.fit(x_train, y_train,batch_size=64,epochs=100,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "trainerr = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train loss:', score[0])\n",
        "print('Train accuracy:', score[1])\n",
        "print('Test loss:',trainerr[0])\n",
        "print('Test accuracy:',trainerr[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETjwNhwsm1EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('losss')\n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}